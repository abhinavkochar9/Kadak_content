# app.py ‚Äî full working Streamlit app (final)
# Features included:
# - reads up to 35 pages from uploaded PDF
# - extracts 10 contextual single-word keywords per page (one line per page)
# - generates songs via Google Gemini (with robust fallback on quota/errors)
# - enforces signature: aesthetic ad-libs + "beyond the notz" immediately after ad-libs,
#   chorus repeated at least 5x, verses short, avoid long formulas & heavy numbers
# - copy buttons for lyrics & AI production prompt
# - caching by PDF hash to avoid repeated API calls during dev/testing
# - keeps UI & flow similar to prior code you provided

import streamlit as st
import google.generativeai as genai
import pdfplumber
from dotenv import load_dotenv
import os
import json
import io
import traceback
import streamlit.components.v1 as components
import hashlib
from datetime import datetime
from collections import Counter
import re

# --- CONFIG & SETUP ---
load_dotenv()
st.set_page_config(page_title="BTN Originals üéß", page_icon="üéπ", layout="wide", initial_sidebar_state="expanded")

# API key handling
if "GOOGLE_API_KEY" in os.environ:
    api_key = os.environ["GOOGLE_API_KEY"]
else:
    api_key = st.sidebar.text_input("Enter Google API Key", type="password")

if api_key:
    try:
        genai.configure(api_key=api_key)
    except Exception as e:
        st.error(f"API Key Error: {e}")

# local in-memory cache: pdf_hash -> {songs, keywords, generated_at}
CACHE = {}

# Simple English + common Hinglish stopwords (extend as needed)
STOPWORDS = {
    "the","and","is","in","to","of","a","an","for","with","on","by","that","this","are","as","it","be",
    "from","at","or","which","have","has","was","were","but","not","they","their","we","you","your",
    # common small words / Hindi connectors
    "ke","ka","ki","hai","mein","se","ko","ya","wo","ho","ye","ke","par","le",
}

# --- HELPERS ---

def hash_bytes(b: bytes) -> str:
    return hashlib.sha1(b).hexdigest()

def extract_text_from_pdf(pdf_file, max_pages=35):
    """
    Reads up to max_pages pages and returns:
    - full_text (joined with page markers)
    - pages_texts (list of each page text)
    """
    try:
        try:
            pdf_file.seek(0)
        except Exception:
            pass
        raw = pdf_file.read()
        if not raw:
            return None, []
        file_obj = io.BytesIO(raw)
        pages_texts = []
        with pdfplumber.open(file_obj) as pdf:
            total = len(pdf.pages)
            pages_to = min(total, max_pages)
            for i in range(pages_to):
                page = pdf.pages[i]
                txt = page.extract_text() or ""
                # normalize whitespace
                txt = re.sub(r'\s+', ' ', txt).strip()
                pages_texts.append(txt)
        full = "\n\n".join(pages_texts)
        return full, pages_texts
    except Exception as e:
        tb = traceback.format_exc()
        st.error(f"Error reading PDF: {e}\n\n{tb}")
        return None, []

def extract_keywords_per_page(pages_texts, top_n=10):
    """
    Simple single-word keyword extractor per page:
    - lowercases, removes punctuation & numbers
    - filters stopwords and short tokens (<3 chars)
    - returns list of lines: each line contains top_n single-word keywords separated by commas
    """
    lines = []
    for ptext in pages_texts:
        # remove numbers & punctuation, keep letters and spaces (retain basic Hindi letters too)
        cleaned = re.sub(r'[^0-9A-Za-z\u0900-\u097F\s]', ' ', ptext)
        tokens = [t.lower() for t in cleaned.split() if t]
        # filter tokens
        filtered = []
        for t in tokens:
            if t in STOPWORDS:
                continue
            if len(t) < 3:
                continue
            # avoid tokens that are mostly digits
            if re.fullmatch(r'\d+', t):
                continue
            filtered.append(t)
        if not filtered:
            lines.append("‚Äî")
            continue
        counts = Counter(filtered)
        # take most common, but ensure uniqueness; choose up to top_n
        top = [w for w,_ in counts.most_common(top_n)]
        # if fewer than top_n, pad with placeholder
        while len(top) < top_n:
            top.append("‚Äî")
        # return as comma-separated single words
        lines.append(", ".join(top[:top_n]))
    return lines

def copy_button_html(text_to_copy):
    js_text = json.dumps(text_to_copy)
    html = f"""
    <button onclick='navigator.clipboard.writeText({js_text})' 
            style="padding:6px 10px;border-radius:6px;border:1px solid #ddd;background:#fff;cursor:pointer;font-weight:600;">
      üìã Copy
    </button>
    """
    return html

# Fallback generator when API quota or errors occur
def fallback_generate_song(text_snippet, style_name="Custom Style"):
    adlibs = "Yeahh! Uh-huh! Aye vibe,"
    chorus = "beyond the notz ‚Äî remember this line"
    # short verses, avoid heavy numbers or long formulas
    verse1 = "Tiny concept, punchline hook, keep it light and bright"
    verse2 = "Mnemonic spark, quick hint ‚Äî make the memory tight"
    lyrics_parts = [adlibs, "beyond the notz", "[CHORUS]"]
    # repeat chorus 5 times (signature)
    for _ in range(5):
        lyrics_parts.append(chorus)
    lyrics_parts += ["[VERSE 1]", verse1, "[VERSE 2]", verse2, "[CHORUS]"]
    lyrics = "\n\n".join(lyrics_parts)
    vibe = f"{style_name} ‚Äî upbeat, melodic-rap soulmate style; classroom-friendly; catchy; short formulas only."
    return {"songs": [{"type": style_name, "title": f"Fallback ‚Äî {style_name}", "vibe_description": vibe, "lyrics": lyrics}]}

# robust json parse attempt
def try_parse_json(raw_text):
    if not raw_text:
        return None
    try:
        return json.loads(raw_text)
    except Exception:
        pass
    start = raw_text.find("{")
    end = raw_text.rfind("}")
    if start != -1 and end != -1 and end > start:
        candidate = raw_text[start:end+1]
        try:
            return json.loads(candidate)
        except Exception:
            pass
    return None

# --- SONG GENERATION (AI call with fallback) ---
def generate_songs(text_content, styles, language_mix, artist_ref, focus_topic, additional_instructions, duration_minutes):
    if not api_key:
        st.error("Missing Google API key. Please enter it in the sidebar.")
        return None

    try:
        model = genai.GenerativeModel("gemini-2.5-flash")
    except Exception as e:
        st.error(f"Model init error: {e}")
        return None

    style_list_str = ", ".join(styles)
    language_instruction = "Balanced Hinglish"
    if language_mix < 30:
        language_instruction = "Mostly Hindi (with English scientific terms)"
    elif language_mix > 70:
        language_instruction = "Mostly English (with Hindi connectors)"

    focus_instruction = f"Focus specifically on this topic: {focus_topic}" if focus_topic else "Cover the most important exam topics from the chapter."
    artist_instruction = f"Take inspiration from the style of: {artist_ref}" if artist_ref else ""
    custom_instructions = f"USER SPECIAL INSTRUCTIONS: {additional_instructions}" if additional_instructions else ""

    if duration_minutes <= 1.5:
        structure = "Quick: CHORUS -> VERSE -> CHORUS -> OUTRO (approx 150 words)"
    elif duration_minutes <= 2.5:
        structure = "Radio: CHORUS -> VERSE -> CHORUS -> VERSE -> CHORUS (200-250 words)"
    elif duration_minutes <= 3.5:
        structure = "Full: Intro/adlibs -> CHORUS -> VERSE -> CHORUS -> VERSE -> CHORUS -> Outro"
    else:
        structure = "Extended: multiple chorus/verse repeats to fit duration"

    source_snippet = text_content[:200000]  # big slice to cover more pages

    # prompt emphasises: adlibs -> 'beyond the notz' immediately, chorus repeated 5x,
    # verses short, avoid long formulas & heavy numeric dumps, keep songs funny & classroom-friendly
    prompt = f"""
You are an expert Gen-Z musical edu-tainer who writes short, funny, punchy, study-friendly songs.
Rules:
- Start with aesthetic ad-libs (examples: "yeahh", "aye vibe", "mmm-hmm").
- Immediately after ad-libs, the NEXT LINE MUST be exactly:
  beyond the notz
- Chorus must include "beyond the notz" at least once and be ultra-catchy. Repeat chorus at least 5 times.
- Keep verses short (<=6 lines each). Avoid long textbook formulas or multi-line derivations‚Äîonly tiny symbolic hints allowed.
- Avoid too many numbers in lyrics.
- Keep songs humorous, relatable, and classroom-safe.

SOURCE (up to 35 pages excerpt):
{source_snippet}

USER:
- Styles: {style_list_str}
- Language mix: {language_instruction}
- Focus: {focus_instruction}
- Artist inspo: {artist_instruction}
- Duration: {duration_minutes} minutes
- Structure guidance: {structure}
- Extra: {custom_instructions}

OUTPUT:
Return ONLY valid JSON in this exact structure:
{{
  "songs": [
    {{
      "type": "Style Name",
      "title": "Creative Song Title",
      "vibe_description": "Suno-style production notes (instruments, BPM, mood)",
      "lyrics": "Full lyrics text (include [CHORUS], [VERSE] labels)."
    }}
  ]
}}
"""
    try:
        resp = model.generate_content(prompt)
    except Exception as e:
        err = str(e).lower()
        # Quota / rate-limit handling -> fallback
        if "quota" in err or "429" in err or "resourceexhausted" in err:
            st.warning("Model quota exceeded or rate-limited. Returning local fallback song to keep UI responsive.")
            return fallback_generate_song(text_content[:3000], styles[0] if styles else "Custom")
        else:
            tb = traceback.format_exc()
            st.error(f"AI call error: {e}\n\n{tb}")
            return fallback_generate_song(text_content[:3000], styles[0] if styles else "Custom")

    # get model text
    try:
        raw_text = resp.text if hasattr(resp, "text") else str(resp)
    except Exception:
        raw_text = str(resp)

    cleaned = raw_text.replace("```json", "").replace("```", "").strip()
    parsed = try_parse_json(cleaned)
    if parsed and isinstance(parsed, dict) and "songs" in parsed:
        # Post-process: make sure signature present; if model missed it, insert minimal fix
        for s in parsed.get("songs", []):
            lyrics = s.get("lyrics", "") or ""
            if "beyond the notz" not in lyrics.lower():
                # insert adlibs + signature at start
                s["lyrics"] = "Yeahh! Uh-huh!\n\nbeyond the notz\n\n" + lyrics
            # reduce long numeric sequences (basic cleanup)
            s["lyrics"] = re.sub(r'\b\d{4,}\b', '', s["lyrics"])
        return parsed
    else:
        st.warning("Model output wasn't valid JSON. Returning fallback formatted song (raw output included).")
        return fallback_generate_song(cleaned[:3000], styles[0] if styles else "Custom")

# --- UI: Sidebar controls ---
st.sidebar.header("üéõÔ∏è Studio Controls")
style_options = ["Desi Hip-Hop / Trap","Punjabi Drill","Bollywood Pop Anthem","Lofi Study Beats","Sufi Rock","EDM / Party","Old School 90s Rap"]
selected_styles = st.sidebar.multiselect("Select Music Styles", options=style_options, default=["Desi Hip-Hop / Trap"])
custom_style_input = st.sidebar.text_input("‚ûï Add Custom Style (Optional)", placeholder="e.g. K-Pop, Ghazal")
st.sidebar.subheader("üó£Ô∏è Language Mixer")
lang_mix = st.sidebar.slider("Hindi vs English", 0, 100, 50)
st.sidebar.subheader("‚è±Ô∏è Track Duration")
duration_minutes = st.sidebar.slider("Length (Minutes)", 1.0, 5.0, 2.5, 0.5)
st.sidebar.subheader("‚ú® Fine Tuning")
artist_ref = st.sidebar.text_input("Artist Inspiration (Optional)", placeholder="e.g. Divine, Arijit Singh")
focus_topic = st.sidebar.text_input("Focus Topic (Optional)", placeholder="e.g. Soaps, Covalent Bonding")
additional_instructions = st.sidebar.text_area("üìù Additional Instructions", placeholder="e.g. keep it funny, short formulas only", height=100)

# --- MAIN UI ---
st.title("üéπ BTN Originals")
st.markdown("Transform NCERT Chapters into Custom Songs ‚Äî signature: *beyond the notz*")

if "song_data" not in st.session_state:
    st.session_state.song_data = None
if "keywords_per_page" not in st.session_state:
    st.session_state.keywords_per_page = None

uploaded_file = st.file_uploader("üìÇ Upload Chapter PDF (up to 35 pages read)", type=["pdf"])

if uploaded_file is not None:
    if st.button("üöÄ Generate Tracks"):
        # compute file hash to use cache
        try:
            uploaded_file.seek(0)
        except Exception:
            pass
        raw_bytes = uploaded_file.read()
        pdf_hash = hash_bytes(raw_bytes)
        # if cached, restore quickly
        if pdf_hash in CACHE:
            cached = CACHE[pdf_hash]
            st.success("Using cached outputs for this PDF.")
            st.session_state.song_data = cached.get("songs")
            st.session_state.keywords_per_page = cached.get("keywords")
            st.experimental_rerun()

        # run extraction & generation
        full_text, pages_texts = extract_text_from_pdf(io.BytesIO(raw_bytes), max_pages=35)
        if not full_text:
            st.error("Failed to extract text from PDF or PDF was empty.")
        else:
            keywords_lines = extract_keywords_per_page(pages_texts, top_n=10)
            with st.spinner("üéß Composing tracks (may take up to 2-3 minutes for long input)..."):
                final_styles = selected_styles.copy()
                if custom_style_input and custom_style_input.strip():
                    if custom_style_input not in final_styles:
                        final_styles.append(custom_style_input)
                if not api_key:
                    st.warning("Please provide a Google API Key in the sidebar.")
                elif not final_styles:
                    st.warning("Please select at least one style.")
                else:
                    result = generate_songs(full_text, final_styles, lang_mix, artist_ref, focus_topic, additional_instructions, duration_minutes)
                    if result:
                        # normalize: sometimes fallback returns dict with 'songs' directly
                        if isinstance(result, dict) and "songs" in result:
                            st.session_state.song_data = result
                            # cache
                            CACHE[pdf_hash] = {"songs": result, "keywords": keywords_lines, "generated_at": datetime.utcnow().isoformat()}
                            st.session_state.keywords_per_page = keywords_lines
                            st.experimental_rerun()
                    else:
                        st.error("No data returned from model. Try again or simplify inputs.")

# --- DISPLAY RESULTS ---
if st.session_state.song_data:
    st.divider()
    st.subheader("üéµ Generated Tracks")
    songs_obj = st.session_state.song_data
    songs = []
    # songs_obj may be a dict or raw; ensure list
    if isinstance(songs_obj, dict) and "songs" in songs_obj:
        songs = songs_obj["songs"]
    elif isinstance(songs_obj, list):
        songs = songs_obj
    else:
        st.error("Unexpected song data format.")
        songs = []

    if not songs:
        st.error("No songs found in the model output.")
    else:
        tabs = st.tabs([s.get('type', f"Track {i+1}") for i, s in enumerate(songs)])
        for i, tab in enumerate(tabs):
            song = songs[i]
            with tab:
                col1, col2 = st.columns([1.6, 1])
                with col1:
                    st.subheader(song.get('title', f"Track {i+1}"))
                    st.markdown("**Lyrics**")
                    st.code(song.get('lyrics', ""), language=None)
                    components.html(copy_button_html(song.get('lyrics', "")), height=44)
                with col2:
                    st.info("üéπ AI Production Prompt")
                    st.markdown(f"_{song.get('vibe_description','')}_")
                    components.html(copy_button_html(song.get('vibe_description', "")), height=44)
                    st.markdown("---")
                    st.success("‚ú® Tip: Paste this prompt into Suno.ai or your DAW.")
                    if st.button("üóëÔ∏è Clear Results", key=f"clear_{i}"):
                        st.session_state.song_data = None
                        st.session_state.keywords_per_page = None
                        st.experimental_rerun()

    # --- KEYWORDS PER PAGE (one line per page, 10 single-word keywords) ---
    st.divider()
    st.subheader("üîé Keywords per page (10 words each ‚Äî page order preserved)")
    if st.session_state.keywords_per_page:
        # show as code block where each line corresponds to page 1..n
        kp_text = "\n".join(st.session_state.keywords_per_page)
        st.code(kp_text, language=None)
        components.html(copy_button_html(kp_text), height=44)
    else:
        st.info("Keywords not available. Generate tracks to extract keywords per page.")
